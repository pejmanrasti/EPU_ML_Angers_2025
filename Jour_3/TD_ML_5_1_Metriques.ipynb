{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c2665898-2c4a-49c3-86fe-16039cc9fbc7",
   "metadata": {},
   "source": [
    "1. D√©finir le probl√®me\n",
    " \n",
    "\n",
    "2. R√©cup√©rer les donn√©es\n",
    " \n",
    "\n",
    "3. Analyser et nettoyer les donn√©es\n",
    "  \n",
    "\n",
    "4. Pr√©parer les donn√©es\n",
    "  \n",
    "\n",
    "5. **Evaluer plusieurs mod√®les**\n",
    "  \n",
    "\n",
    "6. R√©glage fin des mod√®les\n",
    " \n",
    "\n",
    "7. Surveiller son mod√®le"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a24a043-941a-4920-9fc6-72d7af1e111a",
   "metadata": {},
   "source": [
    "# üìå 5. Evaluer plusieurs mod√®les : les m√©triques de performance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0e55b63-69f8-4357-bbaf-4b10d8e2165e",
   "metadata": {},
   "source": [
    "## üöÄ Objectif\n",
    "Dans cette section, nous allons parcourir les **m√©triques de performances** d√©di√©es √† la **classification**.  \n",
    "\n",
    "## üîç √âtapes du processus\n",
    "1. **Importer les biblioth√®ques n√©cessaires, charger les donn√©es, les s√©parer en train/test et les transformer**  \n",
    "2. **Pr√©senter les limites de l'exactitude (Accuracy) en comparant une r√©gression logistique √† un classifieur na√Æf**  \n",
    "3. **Pr√©senter le principe de la matrice de confusion**    \n",
    "4. **Afficher des courbes ROC**  \n",
    "5. **S'attarder sur la probl√©matique du compromis pr√©cision/rappel**   \n",
    "6. **Afficher des courbes de pr√©cision rappel**\n",
    "7. **Faire un bilan**  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65e08ec2-b1c0-4971-94a9-109787d242a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import des biblioth√®ques n√©cessaires\n",
    "import warnings  # Pour √©viter les warnings inutiles\n",
    "\n",
    "import numpy as np  # Pour les calculs num√©riques\n",
    "import pandas as pd  # Pour la manipulation des donn√©es\n",
    "import matplotlib.pyplot as plt  # Pour la visualisation des r√©sultats\n",
    "import seaborn as sns  # Pour am√©liorer l'apparence des graphiques\n",
    "\n",
    "# Scikit-learn : outils pour la pr√©paration et l'√©valuation des mod√®les\n",
    "from sklearn.model_selection import (\n",
    "    train_test_split, cross_val_score, cross_val_predict, StratifiedKFold, GridSearchCV\n",
    ")  # Pour diviser les donn√©es, validation crois√©e et recherche de grille\n",
    "from sklearn.preprocessing import StandardScaler  # Pour la normalisation des donn√©es\n",
    "from sklearn.metrics import (\n",
    "    roc_auc_score, precision_score, recall_score, f1_score, roc_curve, accuracy_score,\n",
    "    ConfusionMatrixDisplay, confusion_matrix, precision_recall_curve, RocCurveDisplay,\n",
    "    PrecisionRecallDisplay, auc\n",
    ")  # Pour l'√©valuation des performances du mod√®le\n",
    "from sklearn.pipeline import Pipeline  # Pour cr√©er des pipelines de traitement\n",
    "from sklearn.feature_selection import SelectKBest, mutual_info_classif  # Pour la s√©lection de variables\n",
    "\n",
    "# Importation des algorithmes de classification\n",
    "from sklearn.linear_model import LogisticRegression  # Pour la r√©gression logistique\n",
    "from sklearn.ensemble import (\n",
    "    RandomForestClassifier, GradientBoostingClassifier, AdaBoostClassifier\n",
    ")  # Classifieurs bas√©s sur des ensembles d'arbres\n",
    "from sklearn.svm import SVC  # Pour les machines √† vecteurs de support\n",
    "from sklearn.neural_network import MLPClassifier  # Pour les r√©seaux de neurones\n",
    "from sklearn.naive_bayes import GaussianNB  # Pour le classifieur Naive Bayes Gaussien\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis  # Pour l'analyse discriminante lin√©aire\n",
    "from sklearn.neighbors import KNeighborsClassifier  # Pour le classifieur des k plus proches voisins\n",
    "from imblearn.under_sampling import RandomUnderSampler  # Pour le sous-√©chantillonnage des donn√©es d√©s√©quilibr√©es\n",
    "from sklearn.base import BaseEstimator  # Classe de base pour cr√©er des estimateurs personnalis√©s\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")  # D√©sactiver les warnings pour ne pas surcharger l'affichage\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98dec0c9-0b0f-4a35-b91e-c77f6d89dd96",
   "metadata": {
    "id": "7jF8v2f5IJH2"
   },
   "source": [
    "## üîπ5.1. Chargement des donn√©es\n",
    "Nous chargeons notre dataset qui contient plusieurs variables explicatives et une variable cible `Echec : 1/0`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70ad2dff-45e5-4df6-9e7c-90099fdf02e9",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 444
    },
    "id": "f9c84e17",
    "outputId": "afdb30b6-3ac7-482a-cc06-ee5e2ee0bfe9"
   },
   "outputs": [],
   "source": [
    "# Chargement des donn√©es\n",
    "df = pd.read_csv(\"/content/DataSet_RegionPelvienne_Class.csv\", sep=\",\")\n",
    "df = df.drop(columns=[\"Unnamed: 0\"])  # Suppression de colonnes inutiles\n",
    "\n",
    "# Aper√ßu des donn√©es\n",
    "display(df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e5f1c43-a5a1-489b-ae0e-45dc60f50f2d",
   "metadata": {
    "id": "0d597cd7"
   },
   "source": [
    "### ‚û°Ô∏è S√©paration des donn√©es en entra√Ænement et test\n",
    "Nous s√©parons le dataset en :\n",
    "- **70% d'entra√Ænement** pour apprendre au mod√®le.\n",
    "- **30% de test** pour √©valuer ses performances sur des donn√©es jamais vues.\n",
    "\n",
    "### ‚û°Ô∏è Sous-√©chantillonnage de la classe majoritaire\n",
    "Nous √©quilibrons la base pour √©viter un d√©s√©quilibre qui pourrait biaiser l'entra√Ænement.\n",
    "\n",
    "### ‚û°Ô∏è Normalisation des variables\n",
    "Les mod√®les sensibles aux √©chelles de variables (**SVM, MLP, R√©gression Logistique**) n√©cessitent une normalisation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae08df01-ed6e-4581-b968-e4525b15e61b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# S√©paration des variables explicatives et de la cible\n",
    "y = df['Echec']\n",
    "X = df.drop(columns=['Echec'])\n",
    "\n",
    "# Division des donn√©es en entra√Ænement (70%) et test (30%)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, stratify=y, random_state=2025)\n",
    "\n",
    "# Sous-√©chantillonnage de la classe majoritaire pour √©quilibrer les classes\n",
    "rus = RandomUnderSampler(sampling_strategy={0: len(y_train[y_train == 1]) * 2, 1: len(y_train[y_train == 1])}, random_state=2025)\n",
    "X_train_res, y_train_res = rus.fit_resample(X_train, y_train)\n",
    "\n",
    "# Normalisation des variables\n",
    "scaler = StandardScaler()\n",
    "X_train_res_std = scaler.fit_transform(X_train_res)  # Normalisation des donn√©es d'entra√Ænement\n",
    "X_test_std = scaler.transform(X_test)  # Normalisation des donn√©es test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0a51770-b02f-4d14-ae74-3043e65273a1",
   "metadata": {},
   "source": [
    "## üîπ 5.2. Les m√©triques de performance\n",
    "Lorsqu'on entra√Æne un mod√®le de ML, il est essentiel d'√©valuer ses performances correctement. Cependant, **toutes les m√©triques ne sont pas adapt√©es** √† tous les probl√®mes.\n",
    "\n",
    "Dans Scikit-Learn, la **m√©trique par d√©faut est souvent l'accuracy**. Or, cette m√©trique peut √™tre trompeuse pour les probl√®mes de classification en cas de **d√©s√©quilibre de classes**.\n",
    "\n",
    "**Exemple** : Si 90% des exemples appartiennent √† la classe 0, un mod√®le qui pr√©dit toujours 0 aura **90% d'accuracy**, mais il ne d√©tectera jamais la classe minoritaire.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26a55f86-aea7-4889-9a45-f4ce9222791b",
   "metadata": {},
   "source": [
    ">Nous allons d√©crire ici les **m√©triques adapt√©es √† la classification binaire**. D'autres m√©triques doivent √™tre utilis√©es si on veut faire de la **classification multiple** ou de la **r√©gression** (pr√©dire directement les valeurs des indices gamma dans notre exemple, au lieu du statut pass/fail).   \n",
    "Les m√©triques disponibles dans scikit peuvent √™tre retrouv√©es ici : üîó [Documentation Scikit - Metriques](https://scikit-learn.org/stable/modules/model_evaluation.html)     \n",
    "Des m√©triques personnalis√©es peuvent aussi √™tre cr√©√©es manuellement : üîó [Documentation Scikit - MetriquesPerso](https://scikit-learn.org/stable/developers/develop.html#rolling-your-own-estimator)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4ecdc7e-73de-482c-bf63-3367d1c76ea5",
   "metadata": {},
   "source": [
    "### üîç Comparaison entre une r√©gression logistique et un classifieur na√Øf\n",
    "Nous allons comparer :\n",
    "1. Une **r√©gression logistique**.\n",
    "2. Un **classifieur na√Øf** qui pr√©dit toujours la classe majoritaire.\n",
    "\n",
    "Nous utiliserons plusieurs **m√©triques de performance** :\n",
    "- **Accuracy** : Combien de pr√©dictions sont correctes parmi toutes les pr√©dictions ?\n",
    "- **Pr√©cision** (Precision) : Combien de pr√©dictions positives sont correctes ?\n",
    "- **AUC-ROC** : Capacit√© du mod√®le √† distinguer les classes positives et n√©gatives (vrais positifs en fonction des faux positifs). AU-ROC = 1 -> Pr√©dicteur parfait\n",
    "  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "396fa217-c2e3-44ea-b867-d569751dd233",
   "metadata": {},
   "source": [
    "**Exemple avec une r√©gression logistique**\n",
    "\n",
    "![image.png](https://datatab.fr/assets/tutorial/Logistic-function.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b888a2d2-59bc-4505-aa0a-7b37df7b6e0f",
   "metadata": {},
   "source": [
    "La fonction logistique a la capacit√© de transformer un nombre compris entre ‚Äì l‚Äôinfini et + l‚Äôinfini en un nombre compris entre 0 et 1 qui se comporte comme une probabilit√©. La r√©gression logistique ne permet de r√©soudre que des probl√®mes de classification. \n",
    "\n",
    "Vid√©o StatQuest qui explique la regression logistique :\n",
    "üîó [Video StatQuest - Regression Logistique](https://www.youtube.com/watch?v=yIYKR4sgzI8)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e50b9b01-3a1f-4179-a411-60041cfcb702",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mod√®le 1 : R√©gression Logistique\n",
    "# --------------------------------\n",
    "# Cr√©ation d'un mod√®le de r√©gression logistique standard.\n",
    "# La r√©gression logistique est un mod√®le de classification binaire bas√© sur une fonction logistique.\n",
    "# Ici, on initialise le mod√®le avec une graine al√©atoire fix√©e pour garantir la reproductibilit√©.\n",
    "clf_lr = LogisticRegression(random_state=2025)\n",
    "\n",
    "# Entra√Ænement du mod√®le de r√©gression logistique sur les donn√©es normalis√©es et r√©√©chantillonn√©es.\n",
    "# - X_train_res_std : Variables explicatives normalis√©es\n",
    "# - y_train_res : Variable cible (√©chec ou r√©ussite)\n",
    "clf_lr.fit(X_train_res_std, y_train_res)\n",
    "\n",
    "# Mod√®le 2 : Classifieur na√Øf (pr√©dit toujours la classe majoritaire)\n",
    "# -------------------------------------------------------------------\n",
    "# Cr√©ation d'un classifieur \"stupide\" qui classe syst√©matiquement toutes les observations\n",
    "# dans la classe majoritaire (ici, classe 0, correspondant √† \"r√©ussite\").\n",
    "# - On utilise la classe `BaseEstimator` de Scikit-learn pour rendre ce mod√®le compatible\n",
    "#   avec les outils d'√©valuation de Scikit-learn (ex: cross-validation).\n",
    "class TjsReussite(BaseEstimator):\n",
    "    def fit(self, X, y=None):\n",
    "        # Pas d'entra√Ænement n√©cessaire, le mod√®le ne fait que des pr√©dictions constantes\n",
    "        pass\n",
    "\n",
    "    def predict(self, X):\n",
    "        # Retourne un tableau rempli de 0 (classe majoritaire) de la m√™me taille que X\n",
    "        return np.zeros(len(X), dtype=int)\n",
    "\n",
    "# Initialisation du classifieur na√Øf\n",
    "clf_stupid = TjsReussite()\n",
    "\n",
    "# Pr√©dictions sur les donn√©es test\n",
    "# --------------------------------\n",
    "# Le mod√®le de r√©gression logistique pr√©dit les classes sur les donn√©es test normalis√©es.\n",
    "y_pred_lr = clf_lr.predict(X_test_std)\n",
    "\n",
    "# Le classifieur na√Øf pr√©dit √©galement les classes sur les m√™mes donn√©es test.\n",
    "y_pred_stupid = clf_stupid.predict(X_test_std)\n",
    "\n",
    "# Calcul des m√©triques de performance\n",
    "# -----------------------------------\n",
    "# D√©finition d'un dictionnaire contenant plusieurs m√©triques d'√©valuation :\n",
    "# - \"Accuracy\" : Exactitude, proportion de pr√©dictions correctes\n",
    "# - \"Pr√©cision\" : Nombre de pr√©dictions positives correctes sur toutes les pr√©dictions positives\n",
    "# - \"AUC-ROC\" : Aire sous la courbe ROC, mesure de la capacit√© du mod√®le √† distinguer les classes\n",
    "metrics = {\n",
    "    \"Accuracy\": accuracy_score,\n",
    "    \"Pr√©cision\": precision_score,\n",
    "    \"AUC-ROC\": roc_auc_score\n",
    "}\n",
    "\n",
    "# Affichage des performances des deux mod√®les\n",
    "print(\"\\nüìä Comparaison des mod√®les\")\n",
    "for metric_name, metric_func in metrics.items():\n",
    "    # √âvaluation de chaque m√©trique pour la r√©gression logistique et le classifieur na√Øf\n",
    "    print(f\"{metric_name} (R√©gression Logistique): {metric_func(y_test, y_pred_lr):.2f}\")\n",
    "    print(f\"{metric_name} (Classifieur na√Øf) : {metric_func(y_test, y_pred_stupid):.2f}\\n\")\n",
    "\n",
    "# Interpr√©tation :\n",
    "# - Si la base de donn√©es est tr√®s d√©s√©quilibr√©e (ex: 90% de classe 0 et 10% de classe 1),\n",
    "#   alors un classifieur qui pr√©dit syst√©matiquement la classe 0 peut obtenir une *accuracy* √©lev√©e\n",
    "#   mais une *pr√©cision* et un *AUC-ROC* tr√®s faibles.\n",
    "# - L'AUC-ROC est g√©n√©ralement une meilleure mesure que l'accuracy pour √©valuer les mod√®les dans un contexte de classification d√©s√©quilibr√©e.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c542d85a-a787-458a-a87d-6ec6c77b12bd",
   "metadata": {},
   "source": [
    "<font size = 7>‚ö†Ô∏è</font>\n",
    "\n",
    "L'accuracy conduit √† **une illusion de performance** si l'on ne regarde pas d'autres m√©triques.   \n",
    "Elle est souvent utilis√©e par d√©faut dans **Scikit-Learn** pour √©valuer les mod√®les, mais elle peut √™tre trompeuse, surtout en **classification avec classes d√©s√©quilibr√©es**. Si elle est utilis√©e pour rechercher les hyper-param√®tres du mod√®le cela peut **fausser le choix des hyper-param√®tres**. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4609c727-cb6b-4375-b570-6744fe651ee7",
   "metadata": {},
   "source": [
    "### üîç Matrice de confusion\n",
    "Une matrice de confusion permet d'√©valuer la qualit√© des pr√©dictions.\n",
    "Elle affiche :\n",
    "- **Vrais positifs (TP)**\n",
    "- **Faux positifs (FP)**\n",
    "- **Vrais n√©gatifs (TN)**\n",
    "- **Faux n√©gatifs (FN)**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fb91515-0d3d-4f82-9526-012a5d63cee9",
   "metadata": {},
   "source": [
    "<img src=\"https://miro.medium.com/v2/resize:fit:969/1*d0UCCIF10Soi7VQGxdVrWQ.jpeg\" alt=\"drawing\" width=\"700\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77648953-010e-4503-be92-9b118de39b52",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Matrice de confusion pour la r√©gression logistique\n",
    "cm_lr = confusion_matrix(y_test, y_pred_lr)\n",
    "ConfusionMatrixDisplay(confusion_matrix=cm_lr, display_labels=[0, 1]).plot()\n",
    "plt.title(\"Matrice de confusion - R√©gression Logistique\")\n",
    "plt.show()\n",
    "\n",
    "# Matrice de confusion pour le classifieur na√Øf\n",
    "cm_stupid = confusion_matrix(y_test, y_pred_stupid)\n",
    "ConfusionMatrixDisplay(confusion_matrix=cm_stupid, display_labels=[0, 1]).plot()\n",
    "plt.title(\"Matrice de confusion - Classifieur Na√Øf\")\n",
    "plt.show()\n",
    "\n",
    "# Matrice de confusion pr√©dictions parfaites\n",
    "cm_stupid = confusion_matrix(y_test, y_test)\n",
    "ConfusionMatrixDisplay(confusion_matrix=cm_stupid, display_labels=[0, 1]).plot()\n",
    "plt.title(\"Matrice de confusion - Classifieur parfait\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43524c61-7c2e-4b29-bb7d-23c3b28c58cd",
   "metadata": {},
   "source": [
    "A partir de la matrice de confusion on a acc√®s √† diff√©rentes m√©triques : "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b645d3e8-575b-4980-a483-3c5d17f9dc51",
   "metadata": {},
   "source": [
    "üî¢ **Notations :**\n",
    "- **TP (True Positives)** : Nombre de cas positifs (fail) correctement d√©tect√©s.\n",
    "- **TN (True Negatives)** : Nombre de cas n√©gatifs (pass) correctement d√©tect√©s.\n",
    "- **FP (False Positives)** : Nombre de cas n√©gatifs (pass) incorrectement d√©tect√©s comme positifs (fail).\n",
    "- **FN (False Negatives)** : Nombre de cas positifs (fail) incorrectement d√©tect√©s comme n√©gatifs (pass).\n",
    "- **P (Positifs r√©els)** : $ P = TP + FN $ (*total des vrais positifs (fail)*).\n",
    "- **N (N√©gatifs r√©els)** : $ N = TN + FP $ (*total des vrais n√©gatifs (pass)*).\n",
    "\n",
    "---\n",
    "\n",
    " üìà **Formules des m√©triques**\n",
    "\n",
    "| **M√©trique**                          | **Formule**                          | **Interpr√©tation** |\n",
    "|----------------------------------------|--------------------------------------|--------------------|\n",
    "| **Probabilit√© de fausse alarme**<br> (*False Positive Rate - FPR*) | $$ \\text{FPR} = \\frac{FP}{N} = 1 - \\text{TNR} $$ | Proportion de cas n√©gatifs <br> incorrectement d√©tect√©s comme positifs. |\n",
    "| **Taux d'√©chec** <br>(*False Negative Rate - FNR*) | $$ \\text{FNR} = \\frac{FN}{P} = 1 - \\text{TPR} $$ | Proportion de cas positifs<br> incorrectement d√©tect√©s comme n√©gatifs. |\n",
    "| **Sensibilit√©** <br>(*Rappel / True Positive Rate - TPR*) | $$ \\text{TPR} = \\frac{TP}{P} = 1 - \\text{FNR} $$ | Proportion de cas positifs <br>correctement d√©tect√©s. |\n",
    "| **Sp√©cificit√©** <br> (*True Negative Rate - TNR*) | $$ \\text{TNR} = \\frac{TN}{N} = 1 - \\text{FPR} $$ | Proportion de cas n√©gatifs <br>correctement d√©tect√©s. |\n",
    "| **Pr√©cision** <br>(*Positive Predictive Value - PPV*) | $$ \\text{PPV} = \\frac{TP}{TP + FP} = 1 - \\text{FDR} $$ | Proportion de cas positifs correctement d√©tect√©s <br> parmi toutes les d√©tections positives. |\n",
    "| **Taux de fausses d√©couvertes** <br> (*False Discovery Rate - FDR*) | $$ \\text{FDR} = \\frac{FP}{TP + FP} = 1 - \\text{PPV} $$ | Proportion de fausses d√©tections <br>parmi toutes les d√©tections positives. |\n",
    "| **Exactitude** <br> (*Accuracy*) | $$ \\text{Accuracy} = \\frac{TP + TN}{P + N} $$ | Proportion de pr√©dictions correctes <br> parmi l‚Äôensemble des cas. |\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "233d89ea-d9bd-4631-9262-9eb830321c6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extraction des valeurs de la matrice de confusion\n",
    "TN, FP, FN, TP = cm_lr.ravel()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77f7f7d5-68bd-4f5a-8ca4-3839f42127c0",
   "metadata": {},
   "source": [
    "\n",
    "<font size = 4><span style=\"color:#2980b9\"> **1Ô∏è‚É£ Sensibilit√© ou Rappel (Recall, Taux de vrais positifs, TPR - True Positive Rate)** </span></font>\n",
    "\n",
    "\n",
    "   - **D√©finition** : Proportion de cas positifs correctement d√©tect√©s par le mod√®le.  \n",
    "   - **Formule** :  \n",
    "$$ \\text{Rappel} = \\frac{TP}{TP + FN} $$\n",
    "   - **Int√©r√™t** : Utile lorsque l‚Äôobjectif est de **minimiser les faux n√©gatifs**, par exemple en **d√©tection de maladies**.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82a38bb7-676f-4e2b-9e37-1c7923f21218",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "G2DBfTTNpzKt",
    "outputId": "9d4978cc-5ed8-4952-cbb5-a63119e7b0e0"
   },
   "outputs": [],
   "source": [
    "# calcul de la sensibilit√© (ou rappel) √† partir du tableau de la matrice de confusion cm[ligne][colonne]\n",
    "TPR = TP/(TP+FN)*100\n",
    "print(\"Le mod√®le d√©tecte %0.0f%% des √©checs sur les donn√©es test.\" %TPR)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ebc27dd-9d45-4eae-8056-dc39e4dce501",
   "metadata": {},
   "source": [
    "On peut aussi utiliser la fonction scikit : "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15aeabd6-425c-4dbe-987c-27459a701a5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "recall_score(y_test, y_pred_lr)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba4470e1-5345-4e6e-9b6b-c89bd2ddba80",
   "metadata": {},
   "source": [
    "<font size = 4><span style=\"color:#2980b9\"> **2Ô∏è‚É£ Sp√©cificit√© (Taux de vrais n√©gatifs, TNR - True Negative Rate)** </span></font>\n",
    "\n",
    "\n",
    "   - **D√©finition** : Proportion de cas n√©gatifs correctement d√©tect√©s.  \n",
    "   - **Formule** :  \n",
    "$$ \\text{Sp√©cificit√©} = \\frac{TN}{TN + FP} $$\n",
    "   - **Int√©r√™t** : Important lorsque **minimiser les faux positifs** est crucial, comme en **d√©tection de fraudes**.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfc20340-0704-4862-95e5-4087b522c0ac",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "nUp2Eq-eqGB9",
    "outputId": "cb56699e-b2f8-424d-9baf-0c69666a3524"
   },
   "outputs": [],
   "source": [
    "# calcul de la sp√©cificit√©\n",
    "TNR = TN/(FP+TN)*100\n",
    "print(\"Le mod√®le d√©tecte %0.0f%% des r√©ussites sur les donn√©es test.\" %TNR)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1dd0536-0d0e-4964-8c65-616931db1a07",
   "metadata": {},
   "source": [
    "<font size = 4><span style=\"color:#2980b9\"> **3Ô∏è‚É£ Pr√©cision (PPV - Positive Predictive Value)** </font></span>\n",
    "\n",
    "\n",
    "   - **D√©finition** : Proportion de cas pr√©dits positifs qui sont r√©ellement positifs.  \n",
    "   - **Formule** :  \n",
    "$$ \\text{Pr√©cision} = \\frac{TP}{TP + FP} $$\n",
    "   - **Int√©r√™t** : Crucial lorsque **minimiser les fausses alertes est essentiel**, comme en **filtrage anti-spam**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "175d9966-7a3b-4913-b9be-bcd57bc01d8a",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "7cCaGUZQsBgB",
    "outputId": "5569de24-2882-43a3-eef0-b51f56dab70c"
   },
   "outputs": [],
   "source": [
    "# calcul de la pr√©cision\n",
    "PPV = TP / (TP + FP)*100\n",
    "print(\"Parmis les √©checs d√©tect√©s sur les donn√©es test, %0.0f%% sont corrects.\" %PPV)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8ef6196-1be8-47ab-876a-fddfe6761774",
   "metadata": {},
   "source": [
    "On peut aussi utiliser la fonction scikit : "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "820a4e6d-b0bb-4134-b344-83075d80f5d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "precision_score(y_test, y_pred_lr)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "329bd61a-94c5-43fa-bb2a-744d7c24d458",
   "metadata": {},
   "source": [
    "### üîç Courbes ROC (Receiver Operating Characteristic)\n",
    "- **Courbe ROC** : Montre le taux de faux positifs (FPR) vs. le taux de vrais positifs (TPR).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac24fe4f-6f64-4a89-a5db-38607f159333",
   "metadata": {},
   "source": [
    "<img src=\"https://abdatum.com/media/images/curva-roc.png\" alt=\"drawing\" width=\"700\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2ddbe5b-7a37-4df1-b0b3-ef0f4604bd8d",
   "metadata": {},
   "source": [
    "<font size = 3>**Aire Sous la Courbe ROC (AUROC)**   </font>   \n",
    "\n",
    "\n",
    "L‚ÄôAUROC (AUC-ROC) est une mesure synth√©tique de performance :\n",
    "- **AUROC = 1** : Mod√®le parfait.\n",
    "- **AUROC = 0.5** : Mod√®le al√©atoire.\n",
    "- **AUROC < 0.5** : Mod√®le invers√© (il pr√©dit mieux les n√©gatifs que les positifs).\n",
    "\n",
    "\n",
    "<font size = 3> üéØ **Pourquoi utiliser la courbe ROC ?**   </font>   \n",
    "\n",
    "\n",
    "‚úîÔ∏è **Ind√©pendante du seuil** de d√©cision utilis√©.  \n",
    "‚úîÔ∏è **Utile sur des bases d√©s√©quilibr√©es**, contrairement √† l‚Äôaccuracy.  \n",
    "‚úîÔ∏è **Permet de comparer plusieurs mod√®les** en fonction de leur capacit√© de discrimination.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c851b66-a5a2-4716-a3d0-7dd3f34e2ce4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Probabilit√©s de pr√©diction de la classe 1\n",
    "# -----------------------------------------\n",
    "# La plupart des mod√®les de classification binaire (comme la r√©gression logistique)\n",
    "# retournent des probabilit√©s d'appartenance √† chaque classe.\n",
    "# Ici, on r√©cup√®re la probabilit√© d'appartenir √† la classe positive (classe 1).\n",
    "# predict_proba() retourne un tableau √† deux colonnes :\n",
    "# - La premi√®re colonne correspond √† la probabilit√© d'appartenir √† la classe 0.\n",
    "# - La deuxi√®me colonne correspond √† la probabilit√© d'appartenir √† la classe 1.\n",
    "# Nous s√©lectionnons uniquement la colonne de la classe positive (index 1).\n",
    "y_scores_lr = clf_lr.predict_proba(X_test_std)[:, 1]\n",
    "\n",
    "# Calcul des courbes ROC\n",
    "# ----------------------\n",
    "# La fonction roc_curve() calcule le taux de faux positifs (FPR) et le taux de vrais positifs (TPR)\n",
    "# pour diff√©rents seuils de classification.\n",
    "# Ces valeurs permettent de tracer la courbe ROC (Receiver Operating Characteristic).\n",
    "# _ repr√©sente les seuils de d√©cision (que l'on n'affiche pas ici).\n",
    "fpr, tpr, _ = roc_curve(y_test, y_scores_lr)\n",
    "\n",
    "# Trac√© de la courbe ROC\n",
    "# ----------------------\n",
    "plt.figure(figsize=(12,5))  # Cr√©ation d'une figure avec une largeur de 12 et hauteur de 5\n",
    "\n",
    "# Cr√©ation du premier sous-graphique (1 ligne, 2 colonnes, premier graphique)\n",
    "plt.subplot(1,2,1)\n",
    "\n",
    "# Trac√© de la courbe ROC : taux de vrais positifs (TPR) en fonction du taux de faux positifs (FPR)\n",
    "plt.plot(fpr, tpr, label=f\"AUROC = {roc_auc_score(y_test, y_scores_lr):.2f}\")\n",
    "\n",
    "# Ajout d'une ligne diagonale en pointill√©s (classifieur al√©atoire, AUROC = 0.5)\n",
    "plt.plot([0,1], [0,1], linestyle='--', color='grey')\n",
    "\n",
    "# Ajout des labels et titre\n",
    "plt.xlabel(\"Taux de faux positifs (FPR) - 1-Sp√©cificit√©\")\n",
    "plt.ylabel(\"Taux de vrais positifs (TPR) - Sensibilit√©\")\n",
    "plt.title(\"Courbe ROC\")\n",
    "\n",
    "# Ajout de la l√©gende affichant la valeur de l'AUROC (aire sous la courbe)\n",
    "plt.legend()\n",
    "\n",
    "# Affichage de la figure\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8a0e7b0-da6e-46e6-b2f5-9b4c0b3415d2",
   "metadata": {},
   "source": [
    "<font size = 6>‚ÑπÔ∏è</font>    \n",
    "On peut aussi tracer la courbe ROC directement √† l'aide de la fonction `RocCurveDisplay()` de scikit : "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdf5e494-b614-4911-8fb9-2bd3e430e518",
   "metadata": {},
   "outputs": [],
   "source": [
    "RocCurveDisplay.from_predictions(y_test, y_scores_lr, plot_chance_level = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20ad8c2d-c984-4ffa-ba7a-a5973493e5b6",
   "metadata": {},
   "source": [
    "<font size = 5>‚ö†Ô∏è</font>\n",
    "\n",
    "Attention quand on trace la courbe ROC ou qu'on calcule l'aire sous la courbe, il faut toujours utiliser les probabilit√©s pr√©dites d'appartenir √† la classe positive et pas les √©tiquettes pr√©dites directement ! L'erreur est facile √† faire (ex : on utilise `predict()` au lieu de `predict_proba()`) et les fonctions scikit ne renverront pas d'erreur.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14742776-f234-4158-a2d7-5ebcff47ddd9",
   "metadata": {},
   "source": [
    "### üîç Compromis entre pr√©cision et rappel\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dff7cca8-08c7-4fe3-a084-659a05fd419b",
   "metadata": {},
   "source": [
    "<img src=\"https://miro.medium.com/v2/resize:fit:824/0*faCzwVWg5RLqAj1J.png\" alt=\"drawing\" width=\"600\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41945c84-205d-4203-9bbf-f2ffc643be0c",
   "metadata": {},
   "source": [
    "Dans une classification binaire, la majorit√© des classifieurs ne prennent pas directement une d√©cision, mais attribuent d‚Äôabord √† chaque observation un **score** ou une **probabilit√©** d‚Äôappartenance √† la classe positive (ex: \"√©chec\" dans un probl√®me de d√©tection).  \n",
    "\n",
    "<font size = 4><span style=\"color:#2980b9\"> **D√©cision bas√©e sur un seuil** </span></font>    \n",
    "Ce score est ensuite compar√© √† un **seuil de d√©cision** pour attribuer une classe :\n",
    "- Si le score est **sup√©rieur** au seuil ‚Üí l'observation est class√©e dans la classe positive.\n",
    "- Sinon ‚Üí elle est class√©e dans la classe n√©gative.\n",
    "\n",
    "<font size = 4><span style=\"color:#2980b9\"> **Impact du seuil sur la pr√©cision et le rappel** </span></font>      \n",
    "Le choix du seuil influence directement les pr√©dictions :\n",
    "- **Un seuil bas** entra√Æne plus de pr√©dictions positives. Cela **augmente le rappel** ( $\\frac{TP}{TP + FN}$) mais **r√©duit la pr√©cision** (on commet plus de faux positifs).\n",
    "- **Un seuil √©lev√©** entra√Æne moins de pr√©dictions positives. Cela **augmente la pr√©cision** ($\\frac{TP}{TP + FP}$), mais **diminue le rappel** (plus de vrais positifs sont manqu√©s).\n",
    "\n",
    "<font size = 4><span style=\"color:#2980b9\"> **Illustration du compromis** </span></font>      \n",
    "- Un seuil **tr√®s bas** : presque toutes les observations sont class√©es comme positives ‚Üí **rappel √©lev√©, pr√©cision faible**.\n",
    "- Un seuil **tr√®s haut** : seules les observations avec un score tr√®s fort sont class√©es positives ‚Üí **pr√©cision √©lev√©e, rappel faible**.\n",
    "\n",
    "<font size = 4>üí° **Choisir le bon seuil d√©pend des objectifs du probl√®me** :   </font>\n",
    "- Si **les faux n√©gatifs sont co√ªteux** (ex: diagnostic m√©dical), on privil√©gie un **rappel √©lev√©**.\n",
    "- Si **les faux positifs sont critiques** (ex: filtrage anti-spam), on favorise une **pr√©cision √©lev√©e**.\n",
    "- Dans certains cas, on utilise des m√©triques combin√©es comme le **score F1** pour √©quilibrer les deux.\n",
    "\n",
    "üëâ **Le trac√© de la courbe Pr√©cision-Rappel et de la courbe ROC aide √† choisir un seuil optimal selon le contexte.** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f9fe639-f85a-44fd-ac91-2076ccf75a3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calcul des scores de d√©cision pour les donn√©es test\n",
    "# decision_function() retourne un score continu pour chaque observation\n",
    "# Plus le score est √©lev√©, plus le mod√®le est confiant que l'exemple appartient √† la classe positive\n",
    "y_scores = clf_lr.decision_function(X_test_std)\n",
    "\n",
    "# Calcul des pr√©cisions et rappels pour diff√©rents seuils de d√©cision\n",
    "# precision_recall_curve() retourne trois √©l√©ments :\n",
    "# - precisions : liste des pr√©cisions obtenues √† chaque seuil\n",
    "# - recalls : liste des rappels obtenus √† chaque seuil\n",
    "# - thresholds : liste des seuils test√©s (longueur = nombre de points - 1)\n",
    "precisions, recalls, thresholds = precision_recall_curve(y_test, y_scores)\n",
    "\n",
    "# Cr√©ation d'une figure pour tracer les courbes\n",
    "plt.figure(figsize=(10, 5))  # D√©finition de la taille de la figure\n",
    "\n",
    "# Trac√© de la courbe de pr√©cision en fonction du seuil\n",
    "plt.plot(thresholds, precisions[:-1], \"b--\", label=\"Pr√©cision\", linewidth=2)\n",
    "\n",
    "# Trac√© de la courbe de rappel en fonction du seuil\n",
    "plt.plot(thresholds, recalls[:-1], \"g-\", label=\"Rappel\", linewidth=2)\n",
    "\n",
    "# Ajout des axes et d'un titre explicatif\n",
    "plt.xlabel(\"Seuil de d√©cision\", fontsize=12)  # Axe des x : valeurs du seuil\n",
    "plt.ylabel(\"Score\", fontsize=12)  # Axe des y : valeurs de pr√©cision/rappel\n",
    "plt.title(\"Courbes de Pr√©cision et Rappel en fonction du seuil de d√©cision\", fontsize=14)\n",
    "\n",
    "# Ajout d'une grille pour une meilleure lisibilit√©\n",
    "plt.grid(True)\n",
    "\n",
    "# Placement de la l√©gende en dehors de la figure\n",
    "# - bbox_to_anchor=(1,1) permet de placer la l√©gende √† l'ext√©rieur de la figure\n",
    "# - loc=\"upper left\" positionne la l√©gende dans le coin sup√©rieur gauche de la zone d'affichage externe\n",
    "plt.legend(loc=\"upper left\", bbox_to_anchor=(1,1), fontsize=12)\n",
    "\n",
    "# Ajustement des marges pour √©viter que la l√©gende ne coupe la figure\n",
    "plt.tight_layout(rect=[0, 0, 0.85, 1])\n",
    "\n",
    "# Affichage de la figure\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8548490-47b0-4bca-8640-5a298b0c6644",
   "metadata": {},
   "source": [
    "<font size = 6>‚ÑπÔ∏è</font>    \n",
    "\n",
    "\n",
    "\n",
    "La courbe de **pr√©cision** est **irr√©guli√®re**. Cela arrive si, en augmentant le seuil, il se trouve qu'on n'augmente pas le nombre de faux positifs mais uniquement celui de faux n√©gatifs. Etant donn√© que le nombre de pr√©dits positifs diminue quand on augmente le seuil de d√©cision, on peut se retrouver avec des variations importantes de la pr√©cision car le rapport entre vrai positifs et pr√©dits positifs peut varier dans un sens comme dans l'autre.      \n",
    "Le rappel diminue forc√©ment avec l'augmentation du seuil car le nombre de r√©ellement positifs est toujours le m√™me alors que les pr√©dits positifs diminuent forc√©ment si on augmente le seuil de d√©cision.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cc0dd9d-c683-4f5f-974a-e883d597f7a5",
   "metadata": {},
   "source": [
    "><font size = 6>‚ö†Ô∏è</font>  \n",
    ">\n",
    ">Dans Scikit-Learn, `predict_proba()` et `decision_function()` sont deux m√©thodes utilis√©es pour obtenir un score de confiance sur les pr√©dictions d'un mod√®le de classification. Cependant, elles diff√®rent dans leur sortie et leur utilisation.   \n",
    "> - `predict_proba()` retourne les probabilit√©s d'appartenance aux diff√©rentes classes, comprises entre 0 et 1   \n",
    "> - `decision_function()` retourne le score brut de la fonction de d√©cision utilis√©e en interne par l'algorithme. Les valeurs ne pas born√©es, elles peuvent √™tre n√©gatives et tr√®s grandes. Plus la valeur est grande plus le mod√®le est confiant sur l'appartenance √† la classe positive. Si le score est >0 c'est la classe positive qui est pr√©dite, s'il est <0 c'est la classe n√©gative.   \n",
    "> - Comme `predict_proba()`retourne des valeurs comprises entre 0 et 1, cela peut √©craser la distribution des scores, rendant moins pr√©cise la d√©termination du seuil adequat pour ajuster le compromis pr√©cision/rappel.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34b6faae-12e3-4b2f-b840-f8d0cfc157df",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "<b>Application </b> Impact du seuil de d√©cision sur le rappel et la pr√©cision : </div> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d9fa402-31aa-4e71-84dd-58d58b4a825d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import ipywidgets as widgets\n",
    "from IPython.display import display, clear_output\n",
    "\n",
    "# Calcul des scores de d√©cision pour les donn√©es test\n",
    "# decision_function() retourne un score continu pour chaque observation\n",
    "# Plus le score est √©lev√©, plus le mod√®le est confiant que l'exemple appartient √† la classe positive\n",
    "y_test_scores = clf_lr.decision_function(X_test_std)\n",
    "\n",
    "# D√©finition du widget interactif pour ajuster le seuil de d√©cision\n",
    "threshold_slider = widgets.FloatSlider(\n",
    "    value=0.5,  # Seuil initial √† 0.5\n",
    "    min=-10,\n",
    "    max=10,\n",
    "    step=1,\n",
    "    description=\"Seuil\",\n",
    "    continuous_update=True\n",
    ")\n",
    "\n",
    "# Widget d'affichage des r√©sultats\n",
    "output = widgets.Output()\n",
    "\n",
    "def update_threshold(threshold):\n",
    "    \"\"\"Met √† jour la pr√©cision et le rappel en fonction du seuil s√©lectionn√©.\"\"\"\n",
    "    with output:\n",
    "        clear_output(wait=True)  # Nettoie l'affichage pr√©c√©dent\n",
    "        \n",
    "        # Appliquer le seuil pour g√©n√©rer les pr√©dictions binaires\n",
    "        y_pred_thresholded = (y_test_scores >= threshold).astype(int)\n",
    "        \n",
    "        # Calcul des m√©triques avec gestion des erreurs\n",
    "        precision = precision_score(y_test, y_pred_thresholded, zero_division=0)\n",
    "        recall = recall_score(y_test, y_pred_thresholded)\n",
    "        tn, fp, fn, tp = confusion_matrix(y_test, y_pred_thresholded).ravel()\n",
    "        specificity = tn / (tn + fp)\n",
    "        \n",
    "        # Affichage des r√©sultats\n",
    "        print(f\"üîπ Seuil de d√©cision (valeur brute de la fonction de d√©cision) : {threshold:.2f}\")\n",
    "        \n",
    "        print(f\"üî∏ Pr√©cision : {precision:.2f}\")\n",
    "        print(f\"üî∏ Rappel/Sensibilit√© : {recall:.2f}\")\n",
    "\n",
    "        # Trac√© des courbes Pr√©cision et Rappel en fonction du seuil\n",
    "        thresholds = np.linspace(-10, 10, 21)\n",
    "        precisions = [precision_score(y_test, (y_test_scores >= t).astype(int), zero_division=0) for t in thresholds]\n",
    "        recalls = [recall_score(y_test, (y_test_scores >= t).astype(int)) for t in thresholds]\n",
    "\n",
    "        plt.figure(figsize=(8, 5))\n",
    "        plt.plot(thresholds, precisions, label=\"Pr√©cision\", color=\"blue\")\n",
    "        plt.plot(thresholds, recalls, label=\"Rappel\", color=\"green\")\n",
    "        plt.axvline(threshold, color=\"red\", linestyle=\"--\", label=f\"Seuil = {threshold:.2f}\")\n",
    "        plt.xlabel(\"Seuil de d√©cision\")\n",
    "        plt.ylabel(\"Valeur\")\n",
    "        plt.title(\"Impact du seuil de d√©cision sur la pr√©cision et le rappel\")\n",
    "        plt.legend()\n",
    "        plt.grid(True)\n",
    "        plt.show()\n",
    "\n",
    "# Lier le widget au callback\n",
    "widgets.interactive(update_threshold, threshold=threshold_slider)\n",
    "\n",
    "# Affichage des widgets\n",
    "display(threshold_slider, output)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd79a9bf-c942-4fa7-bd06-4a2f82640b9c",
   "metadata": {},
   "source": [
    "### üîç Courbe Pr√©cision-Rappel (PR)\n",
    "\n",
    "\n",
    "La **courbe Pr√©cision-Rappel (PR)** est une repr√©sentation graphique de la relation entre **la pr√©cision ($\\frac{TP}{TP + FP}$) et le rappel ( $\\frac{TP}{TP + FN}$)** pour diff√©rents seuils de d√©cision d'un mod√®le de classification binaire.\n",
    "\n",
    "\n",
    "<font size = 3><span style=\"color:#2980b9\"> **Comment interpr√©ter la courbe PR ?**  </span></font>\n",
    "- Un mod√®le **id√©al** aura une pr√©cision et un rappel √©lev√©s, donc sa courbe PR sera **proche du coin sup√©rieur droit**.\n",
    "- Une **courbe plus haute** signifie un **meilleur compromis** entre la pr√©cision et le rappel.\n",
    "- La **baseline** est la proportion d‚Äôexemples positifs dans l‚Äôensemble des donn√©es : si un mod√®le pr√©dit uniquement la classe positive, il aura cette pr√©cision moyenne.\n",
    "\n",
    "<font size = 3><span style=\"color:#2980b9\"> **Pourquoi utiliser la courbe PR ?**   </span>  </font> \n",
    "\n",
    "\n",
    "‚úî **Meilleure √©valuation pour les bases de donn√©es d√©s√©quilibr√©es** :  \n",
    "   - Contrairement √† la courbe ROC, qui inclut le taux de faux positifs (FPR), la courbe PR est **plus informative** lorsque la classe positive est minoritaire.  \n",
    "   - Si une classe est rare, **une faible FPR ($\\frac{FP}{N} $) peut √™tre trompeuse**, car elle ne refl√®te pas bien la capacit√© du mod√®le √† d√©tecter les vrais positifs.  \n",
    "\n",
    "‚úî **Utile pour ajuster le seuil de d√©cision** :  \n",
    "   - En fonction de l‚Äôapplication, on peut privil√©gier **plus de rappel (moins de faux n√©gatifs)** ou **plus de pr√©cision (moins de faux positifs)**.\n",
    "\n",
    "‚úî **√âvaluation globale avec l'aire sous la courbe (AUPRC - Average Precision Score)** :  \n",
    "   - Comme l'AUC-ROC, **l'aire sous la courbe PR (AUPRC)** permet de comparer facilement plusieurs mod√®les.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "377b26c6-e6c5-47d2-a3fa-16db69dd4f21",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calcul des scores de d√©cision pour les donn√©es test\n",
    "# decision_function() retourne un score continu pour chaque observation\n",
    "# Plus le score est √©lev√©, plus le mod√®le est confiant que l'exemple appartient √† la classe positive\n",
    "y_scores = clf_lr.decision_function(X_test_std)\n",
    "\n",
    "# Calcul des pr√©cisions et rappels pour diff√©rents seuils de d√©cision\n",
    "# precision_recall_curve() retourne trois √©l√©ments :\n",
    "# - precisions : liste des pr√©cisions obtenues √† chaque seuil\n",
    "# - recalls : liste des rappels obtenus √† chaque seuil\n",
    "# - thresholds : liste des seuils test√©s (longueur = nombre de points - 1)\n",
    "precisions, recalls, thresholds = precision_recall_curve(y_test, y_scores)\n",
    "\n",
    "\n",
    "# Calcul de l'aire sous la courbe PR (AUPRC)\n",
    "auprc = auc(recalls, precisions)\n",
    "\n",
    "# Cr√©ation de la figure\n",
    "plt.figure(figsize=(8,6))\n",
    "\n",
    "# Trac√© de la courbe Pr√©cision-Rappel\n",
    "plt.plot(recalls, precisions, label=f\"LR AUPRC = {auprc:.2f}\", color=\"blue\", linewidth=2)\n",
    "\n",
    "# Ajout d'une ligne de base correspondant √† la proportion de la classe positive dans les donn√©es test\n",
    "baseline = np.sum(y_test) / len(y_test)\n",
    "plt.plot([0, 1], [baseline, baseline], linestyle='--', color='grey', label=f\"Baseline AUPRC = {baseline:.2f}\")\n",
    "\n",
    "# Ajout des titres et labels\n",
    "plt.xlabel(\"Rappel (Recall)\")\n",
    "plt.ylabel(\"Pr√©cision (Precision)\")\n",
    "plt.title(\"Courbe Pr√©cision-Rappel\")\n",
    "plt.legend(loc=\"upper right\")\n",
    "\n",
    "# Ajout d'une grille pour faciliter la lecture du graphique\n",
    "plt.grid(True)\n",
    "\n",
    "# Affichage du graphique\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "643e89ff-8727-4708-b7fe-45a1ddc1207a",
   "metadata": {},
   "source": [
    "<font size = 6>‚ÑπÔ∏è</font>    \n",
    "On peut aussi tracer la courbe PR directement √† l'aide de la fonction `PrecisionRecallDisplay()` de scikit :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0fba3ec-4bd6-4a4c-b4aa-39dfb5bdda86",
   "metadata": {},
   "outputs": [],
   "source": [
    "PrecisionRecallDisplay.from_predictions(y_test, y_scores, plot_chance_level = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7d25c71-7776-427c-8dd5-516d1d08b7ce",
   "metadata": {
    "id": "adee217f"
   },
   "source": [
    "## üìå Bilan sur les m√©triques de classification\n",
    "\n",
    "En classification, plusieurs **m√©triques d'√©valuation** permettent d'analyser les performances d'un mod√®le. Chaque m√©trique a ses avantages et ses limites, et leur choix d√©pend souvent du **contexte du probl√®me** et de la distribution des classes.\n",
    "\n",
    "---\n",
    "\n",
    " **1Ô∏è‚É£ Accuracy (Exactitude)**   \n",
    "- Proportion de bonnes pr√©dictions parmi l‚Äôensemble des observations.\n",
    "- **Avantage** : Facile √† interpr√©ter.\n",
    "- **Limite** : Peu pertinent en cas de **d√©s√©quilibre des classes** (ex : un mod√®le qui classe tout en n√©gatif peut avoir une haute accuracy mais √™tre inutile).\n",
    "\n",
    "üìå **Formule** :  \n",
    "$$\n",
    "Accuracy = \\frac{TP + TN}{TP + TN + FP + FN}\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    " **2Ô∏è‚É£ Matrice de confusion**   \n",
    "- Table r√©capitulant les **vrais positifs (TP)**, **faux positifs (FP)**, **vrais n√©gatifs (TN)** et **faux n√©gatifs (FN)**.\n",
    "\n",
    "|               | Pr√©diction Positif | Pr√©diction N√©gatif |\n",
    "|--------------|-------------------|-------------------|\n",
    "| **R√©el Positif (P)**  | TP (Vrai Positif)  | FN (Faux N√©gatif) |\n",
    "| **R√©el N√©gatif (N)**  | FP (Faux Positif)  | TN (Vrai N√©gatif) |\n",
    "\n",
    "---\n",
    "\n",
    " **3Ô∏è‚É£ Pr√©cision (Precision)**   \n",
    "- Proportion de pr√©dictions positives qui sont r√©ellement positives.\n",
    "- **Avantage** : √âvite les **faux positifs**, utile si les **erreurs positives sont co√ªteuses** (ex : d√©tection de fraude).\n",
    "- **Limite** : Ignore les **faux n√©gatifs**.\n",
    "\n",
    "üìå **Formule** :  \n",
    "$$\n",
    "Precision = \\frac{TP}{TP + FP}\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    " **4Ô∏è‚É£ Rappel (Recall ou Sensibilit√©)**   \n",
    "- Proportion de cas positifs r√©ellement d√©tect√©s.\n",
    "- **Avantage** : Utile si l'on veut **minimiser les faux n√©gatifs** (ex : diagnostic m√©dical).\n",
    "- **Limite** : Ignore les **faux positifs**.\n",
    "\n",
    "üìå **Formule** :  \n",
    "$$\n",
    "Recall = \\frac{TP}{TP + FN}\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    " **5Ô∏è‚É£ Score F1**   \n",
    "- Moyenne harmonique entre **pr√©cision et rappel**.\n",
    "- **Avantage** : Bon **compromis entre pr√©cision et rappel** si les deux sont importants.\n",
    "- **Limite** : Ne refl√®te pas bien les d√©s√©quilibres de classe.\n",
    "\n",
    "üìå **Formule** :  \n",
    "$$\n",
    "F1 = 2 \\times \\frac{Precision \\times Recall}{Precision + Recall}\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    " **6Ô∏è‚É£ Courbe ROC & AUROC**   \n",
    "- La **courbe ROC** (Receiver Operating Characteristic) trace **le taux de vrais positifs (TPR)** en fonction du **taux de faux positifs (FPR)**.\n",
    "- AUROC proche de **1** ‚Üí Excellent mod√®le, proche de **0.5** ‚Üí Mod√®le al√©atoire.\n",
    "\n",
    "- **Avantages** : Utile si l'on veut **analyser le compromis** entre sensibilit√© et sp√©cificit√©.  \n",
    "\n",
    "- **Limite** : Peu fiable en cas de **d√©s√©quilibre des classes**.\n",
    "\n",
    "---\n",
    "\n",
    " **7Ô∏è‚É£ Courbe Pr√©cision-Rappel (PR) & AUPRC**   \n",
    "- Utile lorsque **la classe positive est rare**.\n",
    "- Permet d'observer **l'√©volution de la pr√©cision et du rappel** en fonction du seuil de d√©cision.\n",
    "- **AUPRC** (Aire sous la courbe Pr√©cision-Rappel) est une alternative √† l'AUC-ROC.\n",
    "\n",
    "- **Avantages** : Plus adapt√©e aux **probl√®mes avec classes d√©s√©quilibr√©es**.  \n",
    "\n",
    "- **Limite** :  Moins intuitive que la courbe ROC.\n",
    "\n",
    "---\n",
    "\n",
    " **üîπ Comment choisir la bonne m√©trique ?**   \n",
    "\n",
    " \n",
    "| Objectif | M√©trique recommand√©e |\n",
    "|----------|---------------------|\n",
    "| Classes √©quilibr√©es | **Accuracy, AUC-ROC** |\n",
    "| D√©tection des positifs importants | **Rappel (Recall), Courbe PR, AUPRC** |\n",
    "| Minimiser les faux positifs | **Pr√©cision (Precision)** |\n",
    "| Compromis entre pr√©cision et rappel | **Score F1** |\n",
    "| Visualisation globale | **Matrice de confusion, ROC, PR** |\n",
    "\n",
    "---\n",
    "\n",
    "‚úÖ **Toujours comparer plusieurs m√©triques avant de choisir un mod√®le final**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8bf7ac3-3d9a-4453-8a06-ff0b5e120584",
   "metadata": {
    "id": "ac19f481"
   },
   "source": [
    "> **M√©triques d‚Äô√©valuation pour la r√©gression**\n",
    ">\n",
    ">En r√©gression, les m√©triques mesurent l‚Äô√©cart entre les valeurs **pr√©dites** et les valeurs **r√©elles**. \n",
    ">\n",
    ">| **M√©trique** | **D√©finition** | **Avantages** | **Inconv√©nients** |\n",
    ">|-------------|--------------|--------------|--------------|\n",
    ">| **Erreur absolue moyenne (MAE)** | Moyenne des √©carts absolus entre <br> pr√©dictions et valeurs r√©elles. | Facile √† interpr√©ter, robuste <br> aux outliers. | Ne p√©nalise pas fortement <br>les grosses erreurs. |\n",
    ">| **Erreur quadratique moyenne (MSE)** | Moyenne des √©carts au carr√© entre <br> pr√©dictions et valeurs r√©elles. | P√©nalise fortement les grosses erreurs, <br>utile pour ajuster un mod√®le. | Sensible aux outliers, <br>les grandes erreurs sont amplifi√©es. |\n",
    ">| **Racine de l‚Äôerreur quadratique moyenne (RMSE)** | Racine carr√©e du MSE,<br> exprim√©e dans la m√™me <br> unit√© que la variable cible. | Facile √† interpr√©ter, <br>p√©nalise fortement les grandes erreurs. | Peut √™tre trop sensible aux outliers. |\n",
    ">| **Coefficient de d√©termination (R¬≤)** | Indique la proportion de variance <br>expliqu√©e par le mod√®le. | Permet de comparer facilement les mod√®les. | Peut √™tre trompeur si les donn√©es sont <br>non lin√©aires ou bruit√©es. |\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e1f70a3-2758-4969-8db5-74ff5edddd29",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
