{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dbeac507-be5f-40dc-b06c-fc30011c76f8",
   "metadata": {},
   "source": [
    "\n",
    "1. Définir le problème\n",
    " \n",
    "\n",
    "2. Récupérer les données\n",
    " \n",
    "\n",
    "3. **Analyser et nettoyer les données**\n",
    "  \n",
    "\n",
    "4. Préparer les données\n",
    "  \n",
    "\n",
    "5. Evaluer plusieurs modèles\n",
    "  \n",
    "\n",
    "6. Réglage fin des modèles\n",
    " \n",
    "\n",
    "7. Surveiller son modèle"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a21096b9-f9e0-4d4a-a16b-be8048a1dc44",
   "metadata": {},
   "source": [
    "# 📌 3. Analyse et Nettoyage des Données  \n",
    "\n",
    "## 🚀 Objectif\n",
    "Dans cette section, nous allons **analyser et nettoyer** nos données pour nous assurer qu'elles sont exploitables avant de les utiliser dans un modèle de **Machine Learning**.  \n",
    "\n",
    "## 🔍 Étapes du processus\n",
    "1. **Importer les bibliothèques nécessaires**  \n",
    "2. **Charger les données**  \n",
    "3. **Supprimer les variables non contributives**  \n",
    "   - Variables constantes  \n",
    "   - Variables fortement corrélées  \n",
    "4. **Gérer les valeurs aberrantes (outliers)**  \n",
    "5. **Visualiser les données**  \n",
    "6. **Catégoriser les résultats gamma (classification)**  \n",
    "7. **Sauvegarder les bases de données nettoyées**  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4492065a-e7b6-4065-825b-bc47aafa947f",
   "metadata": {},
   "source": [
    "## 🔹 3.1. Importation des bibliothèques nécessaires"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c774a5d9-ea41-4264-82fa-0b9495162cd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gestion des avertissements\n",
    "import warnings  # Ignore certains avertissements inutiles lors de l'exécution du code\n",
    "\n",
    "\n",
    "import pandas as pd  # Gestion des DataFrames\n",
    "import numpy as np  # Calculs numériques\n",
    "import math  # Fonctions mathématiques\n",
    "import seaborn as sns  # Graphiques avancés, sert essentiellement à améliorer l'esthétique des graphiques\n",
    "import matplotlib.pyplot as plt  # Affichage de graphiques\n",
    "from pandas.plotting import scatter_matrix  # Matrices de nuages de points\n",
    "from sklearn.feature_selection import VarianceThreshold  # Suppression de variables constantes\n",
    "from scipy import stats  # Tests statistiques\n",
    "from numpy import percentile  # Calcul des percentiles\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")  # Désactiver les warnings pour ne pas surcharger l'affichage"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "461f7d51",
   "metadata": {
    "id": "461f7d51"
   },
   "source": [
    "## 🔹 3.2. Chargement des données\n",
    "\n",
    "Nous utilisons le fichier **`DataSet_RegionPelvienne.csv`** généré précédemment.  \n",
    "Nous supprimons la colonne `Unnamed: 0` qui correspond aux anciens index inutiles.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb64f090",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 256
    },
    "id": "fb64f090",
    "outputId": "87d6e113-b103-413e-c69a-fdfb9fe10a39"
   },
   "outputs": [],
   "source": [
    "# Chargement du fichier CSV\n",
    "df = pd.read_csv(\"/content/DataSet_RegionPelvienne.csv\", sep = \",\", header=0)\n",
    "\n",
    "df = df.drop(columns = [\"Unnamed: 0\"]) #Colonne créée avec les anciens index. On n'en a pas besoin donc on la supprime directement ici.\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55b57da7-9cd8-469a-8ef1-7e2599e3e88d",
   "metadata": {},
   "source": [
    "## 🔹 3.3. Suppression des données non contributives  \n",
    "\n",
    "Certaines variables **n'apportent pas d'information utile** et peuvent être supprimées :\n",
    "1. **Variables constantes** : colonnes avec une variance de 0  \n",
    "2. **Variables fortement corrélées** : colonnes avec informations redondantes\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "Ppd3qIhLXtW9",
   "metadata": {
    "id": "Ppd3qIhLXtW9"
   },
   "source": [
    "### ✂️ Suppression des variables constantes  \n",
    "\n",
    "\n",
    "\n",
    "Nous utilisons la méthode **`VarianceThreshold()`** pour identifier et supprimer les variables constantes.  \n",
    "Par défaut, le seuil est fixé à 0 => la méthode supprime les colonnes dont la variance est à 0 => les colonnes de données constantes. Néanmoins, on pourrait aussi souhaiter supprimer les colonnes dont les données varient très peu donc avec un seuil non nul. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4479a6b-e905-4fe8-ba7f-3a68bea04a26",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialisation du filtre de variance\n",
    "vt = VarianceThreshold() #seuillage par défaut, pas de modification des paramètres. \n",
    "\n",
    "# On applique le seuillage défini dans vt à notre dataframe df avec la méthode .fit. \n",
    "_ = vt.fit(df)  # a noter : le _ permet d'accéder à la volée à la valeur du résultats précédent. Equivalent ici à vt = vt.fit(df).\n",
    "\n",
    "# Obtention des colonnes non contributives\n",
    "mask = vt.get_support() #méthode .get_support() de la classe VarianceThreshold pour obtenir un masque booléen des variables sélectionnées (True si conservées)\n",
    "\n",
    "# Affichage des colonnes supprimées\n",
    "print(\"Variables constantes supprimées :\", list(df.loc[:, ~mask].columns)) # On affiche les noms des colonnes de données qui sont constantes (dans une liste pour faciliter l'affichage)\n",
    "print(\"Total :\", len(mask[mask == False])) # On compte le nombre de variables constantes\n",
    "\n",
    "# Suppression des variables constantes\n",
    "df = df.loc[:, mask] #On ne garde que les variables avec une variance différente de 0\n",
    "\n",
    "# Vérification du DataFrame après suppression\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f2cda6e-d8cc-4db9-868b-f0bd1ebe6339",
   "metadata": {},
   "source": [
    "### ✂️ Suppression des variables fortement corrélées  \n",
    "\n",
    "Les variables **très corrélées** apportent la même information et peuvent perturber l'apprentissage du modèle (plus il y a de variables plus l'entrainement sera long et le modèle risque d'apprendre plus difficilement). Elles peuvent également nuire à l'interprétation du modèle (les variables corrélées seront utilisées aléatoirement de manière équivalente par le modèle, ce qui peut diminuer l'importance de chacune d'elle et au final on peut mal interpréter l'importance de ce que décrivent ces variables pour le modèle quand on cherchera à comprendre comment il fonctionne).    \n",
    "Nous utilisons le **test de normalité de Shapiro-Wilk** pour déterminer le test de corrélation adapté :\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99a92ec5-22c0-4a3e-92b4-b11f1419ac65",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Suppression de la colonne cible pour l'analyse des corrélations entre variables\n",
    "variables = df.drop(columns=['Gamma22loc10'])\n",
    "\n",
    "# Test de Shapiro-Wilk\n",
    "for cm in variables.columns: #cm pour \"complexity metric\"\n",
    "    shapiro_test = stats.shapiro(variables[cm])\n",
    "    p = shapiro_test.pvalue\n",
    "\n",
    "    if p_value > 0.05:\n",
    "        print(cm + \" suit une distribution NORMALE (ne rejette pas H0).\")\n",
    "    else:\n",
    "        print(cm + \" non normale (rejette H0).\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4f02af3-86a3-478d-9e8f-1c1ae76b379d",
   "metadata": {},
   "source": [
    "Le test de Shapiro a tendance être plus adapté pour les taille d'échantillons moyenne (environ 50-1000 exemples). Ici à plus de 1300 échantillons, le test d'**Agostino-Pearson** aurait aussi pu être adapté : "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9986711-e9b5-47ae-9431-dadb50676c7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test de D'Agostino-Pearson\n",
    "for cm in variables.columns:\n",
    "    dagostino_test = normaltest(variables[cm])\n",
    "    p = dagostino_test[1]\n",
    "\n",
    "    if p > 0.05:\n",
    "        print(cm + \" suit une distribution NORMALE (ne rejette pas H0).\")\n",
    "    else:\n",
    "        print(cm + \" non normale (rejette H0).\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82edd95e-698f-436b-9ece-a45843204d9c",
   "metadata": {},
   "source": [
    "Et celui de **Anderson-Darling** particulièrement adapté pour les grands échantillons (>1000 exemples) : "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "873cd10f-be4d-4f3f-a53f-d454fa3d4f77",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test d'Anderson-Darling\n",
    "for cm in variables.columns:\n",
    "    anderson_test = anderson(variables[cm])\n",
    "    p = 0  # Initialisation de la p-valeur\n",
    "\n",
    "    # Déterminer la p-valeur en fonction des niveaux de signification\n",
    "    for i in range(len(anderson_test.significance_level)):\n",
    "        if anderson_test.statistic < anderson_test.critical_values[i]:\n",
    "            p = anderson_test.significance_level[i]\n",
    "            break\n",
    "\n",
    "    if p > 0.05:\n",
    "        print(cm + \" suit une distribution NORMALE (ne rejette pas H0).\")\n",
    "    else:\n",
    "        print(cm + \" non normale (rejette H0).\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c81350f6-9fed-4f9d-9b4b-52432773e1fd",
   "metadata": {},
   "source": [
    "<font size = 4> 🔥 **Matrice de corrélation** </font>    \n",
    "\n",
    "Différents tests de corrélation peuvent être utilisés : \n",
    "- **Pearson** : variables continues et normalement distribuées\n",
    "- **Spearman** et **Kendall** : variables continues ou ordinales, non nécessairement normalement distribuées.\n",
    "- **ANOVA** et **Kruskal-Wallis** : variables mixtes, continues (normales pour ANOVA et non normales pour Kruskal Wallis) et catégorielles. \n",
    "\n",
    "\n",
    "Nous utilisons la **corrélation de Kendall** pour identifier les variables fortement corrélées.  \n",
    "Les valeurs > **0.8** indiquent une redondance forte entre deux variables.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2fc50c1-5526-47ca-bbcf-d729d09fd59c",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-danger\">\n",
    "<b>Attention:</b> La cellule ci-dessous ne peut être exécutée qu'une seule fois. La 2nde fois, elle affichera une erreur car les colonnes demandées ont déjà été supprimées.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c834e26c-9c85-4048-b5e8-72808cdefd94",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calcul de la matrice de corrélation\n",
    "corr_df = variables.corr(method='kendall').abs()\n",
    "\n",
    "# Création d'une matrice triangulaire supérieure\n",
    "upper = corr_df.where(np.triu(np.ones(corr_df.shape), k=1).astype(bool))\n",
    "\n",
    "# Index des colonnes avec un indice de corrélation >0.8\n",
    "# Possibilité de modifier cette valeur pour tester\n",
    "to_drop = [column for column in upper.columns if any(upper[column] > 0.8)]\n",
    "\n",
    "print(\"Variables corrélées supprimées :\", to_drop)\n",
    "print(\"Total :\", len(to_drop))\n",
    "\n",
    "#Pour chaque duo de variables corrélées, on supprime aléatoirement l'une des deux variables concernées. \n",
    "df = df.drop(columns=to_drop)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "gKlMi_G_iCO4",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "gKlMi_G_iCO4",
    "outputId": "b1c28961-53b2-4a7a-f3dc-56f4cdff52d8"
   },
   "outputs": [],
   "source": [
    "#Affichage d'un extrait de la matrice de corrélation\n",
    "#On stocke dans la variable temp quelques unes de nos variables pour l'exemple d'affichage\n",
    "temp = pd.DataFrame(variables, columns =['MUperGy',\n",
    " 'AAV',\n",
    " 'LSV',\n",
    " 'MCS',\n",
    " 'AFW',\n",
    " 'ALT',\n",
    " 'CAS',])\n",
    "corr_temp = temp.corr(method='kendall') #calcul de la matrice de corrélation : correlation entre chaque duo de variable dans le dataframe temp\n",
    "plt.figure(figsize=(8, 8)) # on crée une figure\n",
    "# on crée un masque pour ne garder que la moitié de la matrice (les résultats apparaissent en double sinon, par exemple AAV vs MCS et MCS vs AAV)\n",
    "# np.triu = triangle supérieur d'un tableau, toutes les données sous la diagonale du tableau sont passées à 0\n",
    "# np.ones_like = retourne une matrice de 1 des mêmes dimensions que la matrice donnée en paramètres. Le dtype force le type des variables de sorties du tableau.\n",
    "mask = np.triu(np.ones_like(corr_temp, dtype=bool)) # tableau de 0 (False) sous la diagonale et de 1 (True) au dessus\n",
    "\n",
    "# Heatmap : carte des valeurs de corrélation. Les valeurs ne sont pas affichées quand le masque est \"True\". \n",
    "# Les autres paramètres permettent de régler l'affichage. \n",
    "heatmap = sns.heatmap(corr_temp, mask=mask, vmin=-1, vmax=1, annot=True,annot_kws={\"size\":10}, cmap='BrBG' , linewidth=1, linecolor='w')\n",
    "heatmap.set_title('Matrice de corrélation triangulaire', fontdict={'fontsize':18}, pad=16);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32d52bb3-8b9f-4b60-a6d3-7693f315a0d5",
   "metadata": {},
   "source": [
    ">⚠️ **Le coefficient de corrélation ne mesure que les corrélations linéaires (Spearman, Pearson) ou monotones (Kendall) entre les variables.**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19c1ff60-6130-4018-b63e-35eabe6c08f4",
   "metadata": {},
   "source": [
    "## 🔹 3.4 Gestion des Outliers (valeurs aberrantes des cibles)\n",
    "\n",
    "<font size = 3> 🔍 **Détection** </font>\n",
    "\n",
    "\n",
    "   - **Ecart interquartile** (IQR - Interquartile Range) : on considère qu'une valeur est un outlier si elle est en dehors de l'intervalle `[Q1 - 1.5*IQR, Q3 + 1.5*IQR]` où Q1 est le 1er quartile, Q3 le 3ème quartile et IQR = Q3 - Q1.\n",
    "   - **Z-score** : on considère qu'une valeur est un outlier si sa distance en nombre d'écart-types par rapport à la moyenne est supérieure à 3.   \n",
    "      $ z = \\frac{x - \\mu}{\\sigma} $ où $x$ est la valeur de la donnée, $\\mu$ la moyenne de l'ensemble de données et $\\sigma$ l'écart-type de l'ensemble de donnée. Si $|z|$>3 alors on considère que la valeur est un outlier. \n",
    "    \n",
    "\n",
    "Les **valeurs aberrantes**  des résultats gamma qui peuvent biaiser l'estimation de la limite de tolérance et également l'apprentissage du modèle.  Ces valeurs aberrantes peuvent s'expliquer par des problèmes au moment de la mesure (panne, dérive du détecteur, mauvaise position programmée, etc)    \n",
    "Nous utilisons la méthode **IQR (Interquartile Range)** pour détecter et supprimer les valeurs extrêmes. Cependant, comme les valeurs gamma hors tolérance sont rares et que ce sont elles que l'on veut spécifiquement détecter, il faut faire attention à ne pas supprimer les valeurs avec un seuil trop bas. Nous avons donc choisi d'utiliser la méthode IQR mais avec un seuil à `6*IQR`. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd032db9-e23b-4bc0-84a2-4c37743ac412",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-danger\">\n",
    "<b>Attention:</b> La cellule ci-dessous ne peut être exécutée qu'une seule fois. La 2nde fois, elle n'affichera pas d'erreur mais elle va multiplier les indices gamma une seconde fois par 100 et définir un nouveau seuil de coupure. \n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4399a51d",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "4399a51d",
    "outputId": "adf2397c-2798-445d-9a8c-28f1228ad5e0"
   },
   "outputs": [],
   "source": [
    "# Conversion des indices gamma en pourcentage\n",
    "df.Gamma22loc10 = df.Gamma22loc10*100\n",
    "\n",
    "# Calcul du 1er et 3ème quartile\n",
    "q25, q75 = percentile(df.Gamma22loc10, 25), percentile(df.Gamma22loc10, 75)\n",
    "\n",
    "# Calcul de l'écart interquartile (IQR)\n",
    "iqr = q75 - q25\n",
    "\n",
    "#On affiche les résultats\n",
    "print('Percentiles: 25th=%.3f, 75th=%.3f, IQR=%.3f' % (q25, q75, iqr))\n",
    "\n",
    "# Définition d'un seuil de coupure pour détecter les outliers : on a choisi ici 6*iqr\n",
    "cut_off = iqr * 6\n",
    "lower, upper = q25 - cut_off, q75 + cut_off\n",
    "print(\"cut off = \", cut_off)\n",
    "\n",
    "#On définit les cutoff sup et inf\n",
    "lower, upper = q25 - cut_off, q75 + cut_off\n",
    "\n",
    "print(\"lower cut off = \", lower)\n",
    "\n",
    "# Identification des outliers\n",
    "outliers = [x for x in df.Gamma22loc10 if x < lower or x > upper]\n",
    "\n",
    "print('outliers identifiés : %d' % len(outliers))\n",
    "\n",
    "# suppression des outliers\n",
    "outliers_removed = [x for x in df.Gamma22loc10 if x >= lower and x <= upper]\n",
    "\n",
    "print('Observations non outliers: %d' % len(outliers_removed))\n",
    "\n",
    "df = df[df.Gamma22loc10>= lower]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b850726b-9f7a-4fce-a05a-96b76f4c0f85",
   "metadata": {},
   "source": [
    "## 🔹 3.5 Visualisation des Données  \n",
    "\n",
    "### 🔦 Corrélation des variables avec les cibles  \n",
    "\n",
    "Nous affichons les variables **les plus corrélées** avec `Gamma22loc10`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9ab30c1-e5b8-4e49-b1f0-057799704f62",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calcul de la matrice de corrélation\n",
    "matrice_corr = df.corr(method='kendall').abs()\n",
    "\n",
    "# Affichage des 20 variables les plus corrélées avec Gamma22loc10\n",
    "matrice_corr[\"Gamma22loc10\"].sort_values(ascending=False).head(21)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "l8HsiXy_00gM",
   "metadata": {
    "id": "l8HsiXy_00gM"
   },
   "source": [
    ">⚠️ **Le coefficient de corrélation ne mesure que les corrélations linéaires (Spearman, Pearson) ou monotones (Kendall) entre les variables.**\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "992e19f1-46a1-4990-8bae-75a4a9e6c9f6",
   "metadata": {},
   "source": [
    "### 📊 Distribution des variables  \n",
    "Nous traçons les **distributions des variables les plus importantes**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "SgcATWZn3bKE",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 709
    },
    "id": "SgcATWZn3bKE",
    "outputId": "1e7b7280-0de1-454f-c577-31b5fa0b832f"
   },
   "outputs": [],
   "source": [
    "# Sélection des variables pour la visualisation\n",
    "attributs = [\"Gamma22loc10\", \"EM\", \"BI\", \"EMmin\", \"BIiqr\", \"BImax\", \"BIstd\"]\n",
    "\n",
    "# Matrice de nuages de points\n",
    "scatter_matrix(df[attributs], figsize=(15, 8), diagonal='kde')\n",
    "\n",
    "# Ajustement des axes\n",
    "for i in range(1, 7):\n",
    "    plt.gca().set_ylim(95, 100)\n",
    "    plt.gca().set_xlim(95, 100)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b882e618-afc9-4be6-90d2-db4b63bee5cf",
   "metadata": {},
   "source": [
    "## 🔹 3.6 Catégorisation des résultats PSQA  \n",
    "\n",
    "Nous devons transformer `Gamma22loc10` en **classe binaire** pass/fail pour la classification.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b99eed0a",
   "metadata": {
    "id": "b99eed0a"
   },
   "source": [
    "### 🔧 Binarisation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0e05109-54d9-492d-87ae-0736b38e8e47",
   "metadata": {},
   "source": [
    "<span style=\"color:#2980b9\"> **1. On définit un seuil pour la binarisation** </span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de594524",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 546
    },
    "id": "de594524",
    "outputId": "ccfc2514-308c-4dc5-ae47-bf8337e1b11e"
   },
   "outputs": [],
   "source": [
    "# On définit un seuil, par exemple le 10ème percentile de la distribution de gamma\n",
    "seuil = np.percentile(df.Gamma22loc10, 10)\n",
    "\n",
    "#Affichage de la distribution\n",
    "gamma = df.Gamma22loc10\n",
    "plt.figure(figsize=(12,6))\n",
    "g = sns.displot(gamma)\n",
    "plt.xlabel(\"GPR\", size=14)\n",
    "plt.ylabel(\"Nombre d'exemples\", size=14)\n",
    "plt.axvline(x=seuil,color='red',linestyle='dashed',linewidth=1, label='Percentile')\n",
    "plt.legend(bbox_to_anchor=(1.05, 1.0), loc='upper left', prop={'size': 12})\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51a44e92-4c55-43b3-a3d2-efc4a1779244",
   "metadata": {},
   "source": [
    "<span style=\"color:#2980b9\"> **2. On attribue 1 aux observations sous le seuil (celles que l'on veut prédire) et 0 à celles au-dessus** </span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a838f746-aa03-4901-b8ed-ab92891ee0af",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-danger\">\n",
    "<b>Attention:</b> La cellule ci-dessous ne peut être exécutée qu'une seule fois. La 2nde fois, elle écrasera l'enregistrement de df avant binarisation et renverra une erreur car la colonne Gamma22loc10 a été supprimée\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55f4bf46",
   "metadata": {
    "id": "55f4bf46"
   },
   "outputs": [],
   "source": [
    "# Binarisation\n",
    "# Ajout d'une colonne nommée Echec dans laquelle les indices gamma <=seuil sont passés à 1 (classe positive, 'fail'), et ceux >LCLx à 0 (classe négative, 'pass')\n",
    "df = df.assign(Echec=pd.cut(df.Gamma22loc10,\n",
    "                               bins=[0, seuil, 100],\n",
    "                               labels=[1, 0]))\n",
    "\n",
    "# Suppression de la colonne des indices gamma\n",
    "df = df.drop(columns=[\"Gamma22loc10\"])\n",
    "\n",
    "# Attribution du type \"int\" à la colonne Echec qui contient des 0 et des 1. \n",
    "df['Echec'] = df.Echec.astype('int64')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ae3dc7a",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "5ae3dc7a",
    "outputId": "6ed72468-2462-4243-bb0b-a8ab57f36dd8"
   },
   "outputs": [],
   "source": [
    "#Visualisation des données\n",
    "fig = df.hist(xlabelsize=5, ylabelsize=5,figsize=(13,17) )\n",
    "[x.title.set_size(10) for x in fig.ravel()]\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "NLw59qIjtoa1",
   "metadata": {
    "id": "NLw59qIjtoa1"
   },
   "source": [
    "<font size = 4>📊 Informations que l'on peut extraire de ces histogrammes :</font>\n",
    "\n",
    "\n",
    "* **Différences d'echelles** entre les variables\n",
    "* **Forme des distributions** : plusieurs distributions sont fortement dissymétriques.\n",
    "\n",
    "Ces deux paramètres peuvent nuire aux performances des modèles.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56140aaa",
   "metadata": {
    "id": "56140aaa"
   },
   "source": [
    "## 💾 3.6 Sauvegarde des données "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf9db413",
   "metadata": {
    "id": "bf9db413"
   },
   "outputs": [],
   "source": [
    "# Sauvegarde du fichier CSV après binarisation\n",
    "df.to_csv('DataSet_RegionPelvienne_Class.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b088bf99-90b6-43f0-86ec-60e1f3269814",
   "metadata": {
    "id": "1eiu5c5thISy"
   },
   "source": [
    "📌 **Un fichier CSV a été créé et est prêt pour les prochaines étapes du projet** 🚀"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21ed76d2-e243-48fd-aee7-13e3bd4a8b2d",
   "metadata": {},
   "source": [
    "<font size = 6>⚠️</font> \n",
    "\n",
    "**Télécharger le fichier**"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
